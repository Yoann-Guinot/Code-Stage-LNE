{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importation des librairies"
      ],
      "metadata": {
        "id": "5a8E0bLrpqIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2SKDxhupRyo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import scipy.stats as st\n",
        "import pytensor.tensor as tt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valeurs assignées et mesurées"
      ],
      "metadata": {
        "id": "7smhdZgXpwRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valeurs assignées\n",
        "Assigned_Values = pd.DataFrame({\n",
        "    \"Matériau\": [\"DMR 486b\", \"DMR 274g\", \"SRM 1549a\", \"DMR 82c\",\n",
        "                 \"GBW10037\", \"SRM 1869\", \"SRM 1849a\", \"GBW(E)100227\",\n",
        "                 \"108 02 003\"],\n",
        "    \"X\": [4.51, 5.52, 5.91, 8.83, 39.8, 60.6, 65.3, 97, 108],\n",
        "    \"uX\": [0.11, 0.13, 0.195, 0.205, 1.35, 0.65, 2.8, 2.00, 5.00],\n",
        "    \"Laboratoire\": ['CENAM', 'CENAM', 'NIST', 'CENAM', 'NIM', 'KRISS',\n",
        "                    'NIM', 'NIST', 'NIST']})\n",
        "\n",
        "# Valeurs mesurées\n",
        "Measured_Values = np.array([\n",
        "    4.449, 4.433, 4.083, 4.554, 4.646, 4.210,   # DMR-486b (campagne 1)\n",
        "    4.684, 4.806, 4.491, 4.501, 4.486, 4.471,            # (campagne 2)\n",
        "    4.637, 4.528, 4.692, 4.540, 4.501, 4.620,            # (campagne 3)\n",
        "    5.433, 5.334, 5.417, 5.459, 5.445, 5.488,   # DMR-274g\n",
        "    5.598, 5.352, 5.893, 5.692, 5.282, 5.450,\n",
        "    5.575, 5.436, 5.609, 5.586, 5.189, 5.554,\n",
        "    6.031, 5.827, 6.024, 6.027, 5.539, 5.721,   # SMR 1549a\n",
        "    5.592, 5.986, 6.221, 5.981, 5.096, 4.983,\n",
        "    5.369, 5.394, 5.973, 5.909, 5.556, 5.762,\n",
        "    8.724, 9.207, 8.916, 8.853, 8.952, 8.636,   # DMR-82c\n",
        "    9.189, 9.117, 9.179, 8.553, 9.043, 8.979,\n",
        "    8.609, 8.539, 8.549, 8.421, 8.621, 8.166,\n",
        "    36.49, 36.94, 37.98, 36.81, 37.50, 37.45,   # GBW10037\n",
        "    35.01, 35.56, 37.79, 36.14, 35.29, 36.92,\n",
        "    39.25, 39.49, 37.58, 37.20, 39.14, 37.95,\n",
        "    60.53, 59.99, 61.79, 61.63, 58.69, 60.85,   # SRM 1869\n",
        "    61.81, 60.23, 60.11, 61.07, 61.43, 59.44,\n",
        "    59.16, 59.99, 59.68, 59.58, 60.83, 58.72,\n",
        "    65.32, 64.56, 65.48, 65.25, 63.64, 64.71,   # SRM 1849a\n",
        "    61.48, 63.33, 63.02, 63.49, 65.14, 65.15,\n",
        "    64.66, 64.80, 64.42, 63.86, 63.82, 63.54,\n",
        "    98.59, 96.57, 96.76, 96.83, 100.47, 100.75, # GBW(E)10027\n",
        "    98.84, 98.59, 96.48, 94.42, 95.75, 97.97,\n",
        "    100.27, 96.67, 97.02, 97.21, 100.69, 98.80,\n",
        "    100.10, 98.51, 99.05, 101.62, 100.24, 100.30,   # 108-02-003\n",
        "    100.33, 100.29, 101.97, 99.54, 101.29, 101.78,\n",
        "    99.89, 99.20, 98.11, 98.74, 100.74, 99.48\n",
        "]).reshape(-1, 6)"
      ],
      "metadata": {
        "id": "eKiFIK4epywa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tableaux de données principal"
      ],
      "metadata": {
        "id": "ccFq-Mnup-xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "materiaux = np.repeat(Assigned_Values[\"Matériau\"].values, 3)\n",
        "campagnes = np.tile([1, 2, 3], 9)\n",
        "\n",
        "# Tableau de données format long\n",
        "df = pd.DataFrame({\n",
        "    \"Matériau\": np.repeat(materiaux, 6),\n",
        "    \"Campagne\": np.repeat(campagnes, 6),\n",
        "    \"Aliquot\": np.tile(np.repeat(np.arange(1, 4), 2), len(materiaux)),\n",
        "    \"Y\": Measured_Values.flatten()  # by default to flatten row by row\n",
        "}).merge(Assigned_Values, on=\"Matériau\", how=\"left\")\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "Wjj-JSQQp_bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global display"
      ],
      "metadata": {
        "id": "Kzeqz100qJFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
        "plt.figure(figsize=(14, 9))\n",
        "\n",
        "# Error bars\n",
        "for _, row in df.iterrows():\n",
        "    plt.errorbar(row[\"X\"], row[\"Y\"], xerr=row[\"uX\"], fmt='none',\n",
        "                 ecolor='lightgray', alpha=0.4, zorder=0)\n",
        "\n",
        "plot = sns.scatterplot(data=df, x=\"X\", y=\"Y\", hue=\"Matériau\",\n",
        "                       palette=\"colorblind\", s=40, alpha=0.85)\n",
        "\n",
        "# Line y = x\n",
        "x_min, x_max = df[\"X\"].min(), df[\"X\"].max()\n",
        "plt.plot([x_min, x_max], [x_min, x_max], color=\"lightgray\",\n",
        "         linestyle=\"--\", linewidth=2, zorder=0, label=\"y = x\")\n",
        "\n",
        "# Legend\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "by_label = dict(zip(labels, handles))\n",
        "plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1.05, 1),\n",
        "           loc=\"upper left\", title=\"Matériau\")\n",
        "\n",
        "plt.xlabel(\"Valeurs assignées (X)\")\n",
        "plt.ylabel(\"valeurs mesurées (Y)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4QL7pLZqK0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local displays"
      ],
      "metadata": {
        "id": "4W_80iWhqqri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to distribute points along the uncertainty bar for each\n",
        "# material\n",
        "def x_disp(df):\n",
        "    df = df.copy()\n",
        "    for mat in df[\"Matériau\"].unique():\n",
        "        mask = df[\"Matériau\"] == mat\n",
        "        n = mask.sum()\n",
        "        x_central = df.loc[mask, \"X\"].iloc[0]\n",
        "        incert = df.loc[mask, \"uX\"].iloc[0]\n",
        "        margin = 0.15\n",
        "        left = x_central - incert * (1 - margin)\n",
        "        right = x_central + incert * (1 - margin)\n",
        "        offsets = np.linspace(left, right, n)\n",
        "        df.loc[mask, \"X_disp\"] = offsets\n",
        "    return df\n",
        "\n",
        "df_disp = x_disp(df)\n",
        "\n",
        "# Plotting function for each facet\n",
        "def plot_local(data, **kwargs):\n",
        "# **kwargs receives additional keyword\n",
        "# arguments from map_dataframe\n",
        "    ax = plt.gca()\n",
        "    ax.grid(color=\"lightgray\", linestyle=\"--\", linewidth=0.7, alpha=0.5)\n",
        "    for _, row in data.iterrows():\n",
        "        ax.errorbar(row[\"X\"], row[\"Y\"], xerr=row[\"uX\"], fmt='none',\n",
        "                    ecolor='gray', alpha=0.3, zorder=0)\n",
        "    sns.scatterplot(data=data, x=\"X_disp\", y=\"Y\", hue=\"Campagne\",\n",
        "                    style=\"Aliquot\", palette=\"colorblind\", s=80, ax=ax,\n",
        "                    alpha=0.95, legend=False)\n",
        "\n",
        "    # Graph layout partitioning\n",
        "    x0 = data[\"X\"].iloc[0]\n",
        "    incert = data[\"uX\"].iloc[0]\n",
        "    ax.axvline(x0, color=\"black\", linestyle=\":\", alpha=0.4, zorder=0)\n",
        "    ax.set_xlim(x0 - incert, x0 + incert)\n",
        "\n",
        "# Facet grid\n",
        "g = sns.FacetGrid(df_disp, col=\"Matériau\", col_wrap=3, height=5,\n",
        "                  sharex=False, sharey=False)\n",
        "g.map_dataframe(plot_local)\n",
        "g.set_titles(\"{col_name}\")\n",
        "g.set_axis_labels(\"X\", \"Y\")\n",
        "\n",
        "# Legend\n",
        "plt.figure(figsize=(10, 6))\n",
        "legend_plot = sns.scatterplot(data=df_disp, x=\"X_disp\", y=\"Y\",\n",
        "                              hue=\"Campagne\", style=\"Aliquot\",\n",
        "                              palette=\"colorblind\")\n",
        "handles, labels = legend_plot.get_legend_handles_labels()\n",
        "plt.close()\n",
        "# close the temporary figure after extracting legend\n",
        "# elements\n",
        "g.figure.legend(handles, labels, bbox_to_anchor=(1.02, 0.5),\n",
        "                loc=\"center left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7iDmH8oNqsoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uncertainty calculations**\n",
        "\n",
        "Combined standard uncertainty:\n",
        "\n",
        "$$\n",
        "u_{DOE} = \\sqrt{\\frac{N_a \\times N_r \\times \\sigma_c^2 + N_r \\times \\sigma_a^2 + \\sigma_r^2}{N_c \\times N_a \\times N_r}}\n",
        "$$\n",
        "\n",
        "Expanded uncertainty at 95% confidence:\n",
        "\n",
        "$$\n",
        "U_{95} = \\sqrt{(t \\times u_{DOE})^2 + (2 \\times \\sigma_{char})^2}\n",
        "$$\n",
        "\n",
        "Final standard uncertainty:\n",
        "\n",
        "$$\n",
        "u_{inf} = \\frac{U_{95}}{2}\n",
        "$$\n",
        "\n",
        "Symbols:\n",
        "\n",
        "- $N_a$: number of campaigns\n",
        "- $N_r$: number of aliquots\n",
        "- $N_c$: number of replicates\n",
        "- $\\sigma_c$: campaign effect\n",
        "- $\\sigma_a$: aliquot effect\n",
        "- $\\sigma_r$: repeatability\n",
        "- $t$:  coverage factor (Student’s t-distribution)\n",
        "- $\\sigma_{char}$: characterization uncertainty"
      ],
      "metadata": {
        "id": "vIZEGYoeq_Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_a, N_c, N_r = 3, 3, 2\n",
        "\n",
        "sigma_c  = np.array([0, 0, 0, 0.16701, 0, 0, 0, 0.7087, 0])\n",
        "sigma_a = np.array([0, 0, 0.15444, 0, 0.3858, 0.66436, 0, 0, 0])\n",
        "sigma_r = np.array([0.16724, 0.15921, 0.13728, 0.20217, 0.5787, 1.28964,\n",
        "                    1.0978, 0.8952, 1.0251])\n",
        "\n",
        "u_DOE = np.sqrt((N_a * N_r * sigma_c**2 + N_r * sigma_a**2 + sigma_r**2)\n",
        " / (N_c * N_a * N_r))\n",
        "t = np.array([2.1, 2.1, 2.3, 4.3, 2.3, 2.3, 2.1, 4.3, 2.1])\n",
        "sigma_char = np.array([0.090852, 0.1098, 0.12012, 0.180195, 1.286,\n",
        "                       2.01262, 1.996, 0.6714, 1.08542])\n",
        "\n",
        "U_95 = np.sqrt((t * u_DOE)**2 + (2 * sigma_char)**2)\n",
        "\n",
        "u_inf = U_95 / 2\n",
        "\n",
        "# Assigning the calculated uncertainties to the data frame\n",
        "df[\"uY\"] = df[\"Matériau\"].map(dict(zip(Assigned_Values[\"Matériau\"],\n",
        "                                       u_inf)))"
      ],
      "metadata": {
        "id": "K6w1uIKisBTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian model"
      ],
      "metadata": {
        "id": "OSZAK3DjsGEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensions\n",
        "n0, n1, n2, n3 = 9, 3, 3, 2\n",
        "\n",
        "# Données observées\n",
        "X_obs, uX = Assigned_Values[\"X\"].values, Assigned_Values[\"uX\"].values\n",
        "mat_order = Assigned_Values[\"Matériau\"].values\n",
        "Y_obs = df[\"Y\"].groupby(df[\"Matériau\"]).mean().reindex(mat_order).values\n",
        "uY = df[\"uY\"].groupby(df[\"Matériau\"]).mean().reindex(mat_order).values\n",
        "Rdat = Measured_Values.reshape(n0, n1, n2, n3)\n",
        "uRda1 = np.array([0.09, 0.11, 0.12, 0.18, 0.66, 1.10, 1.30, 2.01, 2.03])\n",
        "\n",
        "# Model\n",
        "with pm.Model() as model:\n",
        "    uRda1_data = pm.Data('uRda1', uRda1)\n",
        "    uX_data = pm.Data('uX', uX)\n",
        "    uY_data = pm.Data('uY', uY)\n",
        "\n",
        "    # Paramètres de régression\n",
        "    a = pm.Normal('a', mu = 0, sigma = 1 / np.sqrt(1e-5))\n",
        "    b = pm.Normal('b', mu = 1, sigma = 1 / np.sqrt(1e-5))\n",
        "\n",
        "    # Paramètres instrumentaux\n",
        "    sigma_mat = pm.HalfNormal('sigma_mat', sigma = 1)\n",
        "    smthd = pm.Deterministic('smthd', 100 / pm.math.sqrt(sigma_mat))\n",
        "\n",
        "    prept = pm.Deterministic('prept', sigma_mat / (uX_data ** 2))\n",
        "    srept = pm.Deterministic('srept', 1 / pm.math.sqrt(prept))\n",
        "\n",
        "    # Modèle mesurand\n",
        "    smodel = pm.Normal('smodel', mu=0, sigma = uRda1_data, shape = n0)\n",
        "\n",
        "    # Variables certifiées Vtru et observations Vda1\n",
        "    Vtru = pm.Normal('Vtru', mu = 0, sigma = 1 / np.sqrt(1e-5),\n",
        "                     shape = n0)\n",
        "    Vda1_obs = pm.Normal('Vda1_obs', mu = Vtru, sigma = uX_data,\n",
        "                         observed = X_obs)\n",
        "\n",
        "    # Variables Vhat et observations Vda2\n",
        "    Vhat = pm.Normal('Vhat', mu = 0, sigma = 1 / np.sqrt(1e-5),\n",
        "                     shape = n0)\n",
        "    Vda2_obs = pm.Normal('Vda2_obs', mu = Vhat, sigma = uY_data,\n",
        "                         observed = Y_obs)\n",
        "\n",
        "    # Régression prédictive\n",
        "    alpha = pm.Deterministic('alpha', a + b * Vhat)\n",
        "    Rhatuc = pm.Deterministic('Rhatuc', alpha + smodel)\n",
        "    Vhatuc = pm.Deterministic('Vhatuc', (Rhatuc - a) / b)\n",
        "\n",
        "    # Biais hiérarchiques beta, gamma\n",
        "    sigma_cam = pm.HalfNormal('sigma_cam', sigma = 1, shape = (n0, n1))\n",
        "    beta = pm.Normal('beta', mu = alpha[:, None], sigma = 1 /\n",
        "                     pm.math.sqrt(prept[:, None]), shape = (n0, n1))\n",
        "\n",
        "    sigma_ali = pm.HalfNormal('sigma_ali', sigma = 1,\n",
        "                              shape = (n0, n1, n2))\n",
        "    gamma = pm.Normal('gamma', mu = beta[:, :, None], sigma = 1 /\n",
        "                      pm.math.sqrt(sigma_cam[:, :, None]),\n",
        "                      shape = (n0, n1, n2))\n",
        "\n",
        "    # Observations répétées Rdat\n",
        "    Rdat_obs = pm.Normal('Rdat_obs', mu = tt.repeat(gamma[:, :, :, None],\n",
        "                                                    n3, axis=3),\n",
        "                          sigma = 1 / pm.math.sqrt(tt.repeat(\n",
        "                              sigma_ali[:, :, :, None], n3, axis=3)),\n",
        "                          observed = Rdat)\n",
        "\n",
        "    # Calcul du degré d'équivalence\n",
        "    doe = pm.Deterministic('doe', 200 * (Vtru - Vhatuc)\n",
        "    / (Vtru + Vhatuc))\n",
        "\n",
        "    initvals = {\"sigma_mat\": 1, \"sigma_cam\": np.ones((n0, n1)),\n",
        "                \"sigma_ali\": np.ones((n0, n1, n2))}\n",
        "\n",
        "    trace = pm.sample(draws = 4000, tune = 12000, target_accept = 0.99,\n",
        "                      nuts_sampler = 'numpyro', random_seed = 23,\n",
        "                      initvals = initvals)\n",
        "\n",
        "# Directed Acyclic Graph\n",
        "pm.model_to_graphviz(model)"
      ],
      "metadata": {
        "id": "qoNnScN8sEqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résumé brut de la trace"
      ],
      "metadata": {
        "id": "_Vh7vVvNifLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = az.summary(trace, hdi_prob = 0.95)\n",
        "display(summary)"
      ],
      "metadata": {
        "id": "WguVicZ_sKwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Résumé poli de la trace"
      ],
      "metadata": {
        "id": "UFcTe9O5iiVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = az.summary(trace, var_names = [\"alpha\", \"beta\", \"gamma\",\n",
        "                                         \"sigma_mat\", \"sigma_cam\",\n",
        "                                         \"sigma_ali\"], hdi_prob = 0.95)\n",
        "summary = summary[[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\"]]\n",
        "\n",
        "# Ajout d'alpha_mean\n",
        "alpha = az.extract(trace, var_names = [\"alpha\"])\n",
        "alpha_mean = alpha.mean(dim = \"alpha_dim_0\")\n",
        "row_alpha_mean = {\"mean\": float(alpha_mean.mean()),\n",
        "                  \"sd\": float(alpha_mean.std()),\n",
        "                  \"hdi_2.5%\": np.percentile(alpha_mean, 2.5),\n",
        "                  \"hdi_97.5%\": np.percentile(alpha_mean, 97.5)}\n",
        "extra = pd.DataFrame([row_alpha_mean], index = [\"alpha_mean\"])\n",
        "summary = pd.concat([summary, extra])\n",
        "\n",
        "# Ajout de beta_mean\n",
        "beta = az.extract(trace, var_names = [\"beta\"])\n",
        "beta_mean = beta.mean(dim = \"beta_dim_1\")\n",
        "beta_mean_stats = []\n",
        "for i in range(beta_mean.sizes[\"beta_dim_0\"]):\n",
        "    arr = beta_mean.sel(beta_dim_0 = i).values.flatten()\n",
        "    beta_mean_stats.append({\n",
        "        \"mean\": arr.mean(),\n",
        "        \"sd\": arr.std(),\n",
        "        \"hdi_2.5%\": np.percentile(arr, 2.5),\n",
        "        \"hdi_97.5%\": np.percentile(arr, 97.5)\n",
        "        })\n",
        "extra = pd.DataFrame(\n",
        "    beta_mean_stats,\n",
        "    index = [f\"beta_{i}_mean\" for i in range(len(\n",
        "        beta_mean_stats))])\n",
        "summary = pd.concat([summary, extra])\n",
        "\n",
        "# Ajout de gamma_mean\n",
        "gamma = az.extract(trace, var_names = [\"gamma\"])\n",
        "gamma_mean = gamma.mean(dim = \"gamma_dim_2\")\n",
        "gamma_mean_stats = []\n",
        "for i in range(gamma_mean.sizes[\"gamma_dim_0\"]):\n",
        "    for j in range(gamma_mean.sizes[\"gamma_dim_1\"]):\n",
        "        arr = gamma_mean.sel(\n",
        "            gamma_dim_0 = i, gamma_dim_1 = j).values.flatten()\n",
        "        gamma_mean_stats.append({\n",
        "            \"mean\": arr.mean(),\n",
        "            \"sd\": arr.std(),\n",
        "            \"hdi_2.5%\": np.percentile(arr, 2.5),\n",
        "            \"hdi_97.5%\": np.percentile(arr, 97.5)\n",
        "        })\n",
        "\n",
        "extra = pd.DataFrame(\n",
        "    gamma_mean_stats,\n",
        "    index=[f\"gamma_{i}_{j}_mean\" for i in range(\n",
        "        gamma_mean.sizes[\"gamma_dim_0\"])\n",
        "           for j in range(gamma_mean.sizes[\"gamma_dim_1\"])]\n",
        ")\n",
        "summary = pd.concat([summary, extra])\n",
        "\n",
        "# Ajout de sigma_cam\n",
        "sigma_cam = az.extract(trace, var_names = [\"sigma_cam\"])\n",
        "sigma_cam_mean = (sigma_cam**2).mean(dim = \"sigma_cam_dim_1\")**0.5\n",
        "sigma_cam_mean_stats = []\n",
        "for i in range(sigma_cam_mean.sizes[\"sigma_cam_dim_0\"]):\n",
        "    arr = sigma_cam_mean.sel(sigma_cam_dim_0 = i).values.flatten()\n",
        "    sigma_cam_mean_stats.append({\n",
        "        \"mean\": arr.mean(),\n",
        "        \"sd\": arr.std(),\n",
        "        \"hdi_2.5%\": np.percentile(arr, 2.5),\n",
        "        \"hdi_97.5%\": np.percentile(arr, 97.5)\n",
        "        })\n",
        "extra = pd.DataFrame(sigma_cam_mean_stats, index = [\n",
        "    f\"sigma_cam_{i}_mean\" for i in range(len(sigma_cam_mean_stats))])\n",
        "summary = pd.concat([summary, extra])\n",
        "\n",
        "# Ajout de sigma_ali\n",
        "sigma_ali = az.extract(trace, var_names = [\"sigma_ali\"])\n",
        "sigma_ali_mean = (sigma_ali**2).mean(dim = \"sigma_ali_dim_2\")**0.5\n",
        "sigma_ali_mean_stats = []\n",
        "for i in range(sigma_ali_mean.sizes[\"sigma_ali_dim_0\"]):\n",
        "    for j in range(sigma_ali_mean.sizes[\"sigma_ali_dim_1\"]):\n",
        "        arr = sigma_ali_mean.sel(sigma_ali_dim_0 = i,\n",
        "                                 sigma_ali_dim_1 = j).values.flatten()\n",
        "        sigma_ali_mean_stats.append({\n",
        "            \"mean\": arr.mean(),\n",
        "            \"sd\": arr.std(),\n",
        "            \"hdi_2.5%\": np.percentile(arr, 2.5),\n",
        "            \"hdi_97.5%\": np.percentile(arr, 97.5)\n",
        "        })\n",
        "\n",
        "extra = pd.DataFrame(\n",
        "    sigma_ali_mean_stats,\n",
        "    index=[f\"sigma_ali_{i}_{j}_mean\" for i in range(\n",
        "        sigma_ali_mean.sizes[\"sigma_ali_dim_0\"])\n",
        "           for j in range(sigma_ali_mean.sizes[\"sigma_ali_dim_1\"])]\n",
        ")\n",
        "summary = pd.concat([summary, extra])\n",
        "\n",
        "display(summary)"
      ],
      "metadata": {
        "id": "2jBrEEBEsP2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Régression avec bande de crédibilité"
      ],
      "metadata": {
        "id": "LHUdA6H7i1Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction des postérieurs\n",
        "a_samples = trace.posterior[\"a\"].values.flatten()\n",
        "b_samples = trace.posterior[\"b\"].values.flatten()\n",
        "\n",
        "x_vals = np.linspace(X_obs.min(), X_obs.max(), 100)\n",
        "\n",
        "# Prédictions postérieures\n",
        "y_preds = np.array([a + b * x_vals for a, b in zip(a_samples, b_samples)])\n",
        "y_mean = y_preds.mean(axis = 0)\n",
        "y_hdi_low, y_hdi_high = np.percentile(y_preds, [2.5, 97.5], axis = 0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.fill_between(x_vals, y_hdi_low, y_hdi_high, color = \"gray\",\n",
        "                 alpha = 0.3, label=\"Bande de crédibilité 95%\")\n",
        "plt.plot(x_vals, y_mean, color = \"red\", label = \"Régression bayésienne\",\n",
        "         linewidth = 1, zorder = 1)\n",
        "plt.errorbar(X_obs, Y_obs, xerr = 4 * uX, yerr = 4 * uY, fmt = 'none',\n",
        "             label = \"Données (incertitude x 4)\", color = \"black\",\n",
        "             elinewidth = 1)\n",
        "for i, mat in enumerate(Assigned_Values[\"Matériau\"]):\n",
        "    plt.scatter(X_obs[i], Y_obs[i], label = mat, s = 10, zorder = 2)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\", rotation = 0, labelpad = 50, ha = 'left')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rnP_t-4gsdAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracés des lois a posteriori"
      ],
      "metadata": {
        "id": "eFSEhMg-ipbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.hist(a_samples, bins = 40, color = \"skyblue\", edgecolor = \"k\")\n",
        "plt.title(\"Posterior de a\")\n",
        "plt.xlabel(\"a\")\n",
        "plt.ylabel(\"Densité\")\n",
        "plt.show()\n",
        "\n",
        "plt.hist(b_samples, bins = 40, color = \"salmon\", edgecolor = \"k\")\n",
        "plt.title(\"Posterior de b\")\n",
        "plt.xlabel(\"b\")\n",
        "plt.ylabel(\"Densité\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ItDUsJh-se6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracé de la loi a priori et de celle a posteriori"
      ],
      "metadata": {
        "id": "ymJgIK-9iJyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_a = np.linspace(a_samples.min() - 1, a_samples.max() + 1, 300)\n",
        "x_b = np.linspace(b_samples.min() - 1, b_samples.max() + 1, 300)\n",
        "\n",
        "plt.figure(figsize = (10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(a_samples, bins = 40, color = \"skyblue\", edgecolor = \"k\",\n",
        "         density = True, alpha = 0.7, label = \"Posterior\")\n",
        "plt.plot(x_a, st.norm.pdf(x_a, loc = 0, scale = 316), 'k--',\n",
        "         label = \"Prior théorique\")  # prior de a\n",
        "plt.title(\"Posterior de a\")\n",
        "plt.xlabel(\"a\")\n",
        "plt.ylabel(\"Densité\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(b_samples, bins=40, color = \"salmon\", edgecolor = \"k\",\n",
        "         density = True, alpha = 0.7, label = \"Posterior\")\n",
        "plt.plot(x_b, st.norm.pdf(x_b, loc = 1, scale = 316), 'k--',\n",
        "         label = \"Prior théorique\")  # prior de b\n",
        "plt.title(\"Posterior de b\")\n",
        "plt.xlabel(\"b\")\n",
        "plt.ylabel(\"Densité\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k1Z_dT86s_7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}